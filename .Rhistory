UCModel$est
UCModel <- ucm(temp~0, data = temp, season = TRUE, season.length = 12)
UCModel
UCM_predict = predict(UCModel$model, n.ahead = 12)
ts.plot(temp, UCM_predict, gpars = list(main = "Holt Winter Fitted vs Observed",col = c("blue", "red")))
UCModel <- ucm(temp~0, data = temp, cycle = TRUE, cycle.period =  12)
UCModel
UCM_predict = predict(UCModel$model, n.ahead = 12)
ts.plot(temp, UCM_predict, gpars = list(main = "Holt Winter Fitted vs Observed",col = c("blue", "red")))
plot(temp, ylab = "Flow of Nile")
lines(UCModel$s.cycle, col = "blue")
legend("topright", legend = c("Observed flow","S_level"), col = c("black","blue"), lty = 1)
plot(UCModel$s.cycle, ylab = "Flow of Nile")
UCModel$s.cycle
predict(UCModel$model)
plot(predict(UCModel$model))
UCModel <- ucm(temp~0, data = temp, season = TRUE, season.length =  12)
UCModel
plot(predict(UCModel$model))
UCM_predict = predict(UCModel$model, n.ahead = 12)
ts.plot(temp, UCM_predict, gpars = list(main = "Holt Winter Fitted vs Observed",col = c("blue", "red")))
plot(predict(UCModel$model$Z))
UCModel$model$Z
UCModel$model$Y
UCModel$model$y
UCModel$s.season
UCModel
UCModel$est.var.cycle
UCModel
plot(predict(UCModel$model))
UCModel$s.level
UCModel$s.season
UCModel$model$names
UCModel$model$P1
UCModel$model$u
UCModel <- ucm(temp~0, data = temp, season = TRUE, season.length =  12)
UCModel
plot(predict(UCModel$model))
ts.plot(predict(UCModel$model), temp, gpars = list(main = "Holt Winter Fitted vs Observed",col = c("blue", "red")))
legend("topright", legend = c("Predicted","Observed"), col = c("blue","red"), lty = 1)
UCModel <- ucm(temp~0, data = temp, season = TRUE, season.length =  12, irregular = TRUE)
UCModel
plot(predict(UCModel$model))
ts.plot(predict(UCModel$model), temp, gpars = list(main = "Holt Winter Fitted vs Observed",col = c("blue", "red")))
legend("topright", legend = c("Predicted","Observed"), col = c("blue","red"), lty = 1)
UCModel <- ucm(temp~0, data = temp, season = TRUE, season.length =  12, irregular = TRUE, slope = TRUE)
UCModel
plot(predict(UCModel$model))
ts.plot(predict(UCModel$model), temp, gpars = list(main = "Holt Winter Fitted vs Observed",col = c("blue", "red")))
legend("topright", legend = c("Predicted","Observed"), col = c("blue","red"), lty = 1)
UCModel <- ucm(temp~0, data = temp, season = TRUE, season.length =  12, irregular = TRUE, slope = TRUE, cycle = TRUE)
UCModel <- ucm(temp~0, data = temp, season = TRUE, season.length =  12, irregular = TRUE, slope = TRUE)
UCModel
UCM_predict = predict(UCModel$model, n.ahead = 12)
ts.plot(temp, UCM_predict, gpars = list(main = "Holt Winter Fitted vs Observed",col = c("blue", "red")))
knitr::opts_chunk$set(echo = TRUE)
library(devtools)
library(Rfacebook)
library(scales)
library(reshape2)
library(corrplot)
library(textclean)
library(SnowballC)     # interface to the C libstemmer library that implements Porter's word stemming algorithm for collapsing words to a common root to aid comparison of vocabulary
library(twitteR)
library(tm)     	# text mining
library(syuzhet)     	# extracts sentiment and sentiment-derived plot arcs from text
library(sentimentr)     # sentiment analysis
library(wordcloud)
library(plyr)
library(ggplot2)
# Generate an access token to obtain your Twitter Access Token and Access Token Secret.
consumer_key    <- "BkiIHNzE2BvQhbRYVpkbZgGTA"
consumer_secret <-"zHAcSayUgKlfkksuf57oB0CMqFwDtgGI4TkDurXkEgNjHsqpnx"
access_token  <- "991503034650976257-W3MQKW99A0ec2pcm3kBw6ddXBIwkji5"
access_secret <- "bjhEqauHjM2gLPn7qUJvzThTWUW0vN3XVR8AkxsbphbD7"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)     # Setup Twitter open authorization. Select 2.
## Twitter Sentiment Analysis
tweet1 <- searchTwitter("Deadpool", n=18000, lang="en", since = '2018-05-16', until = '2018-05-16')     # Pull Twitter feed for Arsenal.
tweet1 <- twListToDF(tweet1)     # Convert into a data frame for analysis.
knitr::opts_chunk$set(echo = TRUE)
library(devtools)
library(Rfacebook)
library(scales)
library(reshape2)
library(corrplot)
library(textclean)
library(SnowballC)     # interface to the C libstemmer library that implements Porter's word stemming algorithm for collapsing words to a common root to aid comparison of vocabulary
library(twitteR)
library(tm)     	# text mining
library(syuzhet)     	# extracts sentiment and sentiment-derived plot arcs from text
library(sentimentr)     # sentiment analysis
library(wordcloud)
library(plyr)
library(ggplot2)
# Generate an access token to obtain your Twitter Access Token and Access Token Secret.
consumer_key    <- "BkiIHNzE2BvQhbRYVpkbZgGTA"
consumer_secret <-"zHAcSayUgKlfkksuf57oB0CMqFwDtgGI4TkDurXkEgNjHsqpnx"
access_token  <- "991503034650976257-W3MQKW99A0ec2pcm3kBw6ddXBIwkji5"
access_secret <- "bjhEqauHjM2gLPn7qUJvzThTWUW0vN3XVR8AkxsbphbD7"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)     # Setup Twitter open authorization. Select 2.
## Twitter Sentiment Analysis
tweet1 <- searchTwitter("Deadpool", n=10, lang="en")     # Pull Twitter feed for Arsenal.
tweet1 <- searchTwitter("Deadpool", n=1, lang="en")     # Pull Twitter feed for Arsenal.
tweet1 <- searchTwitter("Deadpool", n=1, lang="en")     # Pull Twitter feed for Arsenal.
tweet1 <- twListToDF(tweet1)     # Convert into a data frame for analysis.
View(tweet1)
tweet1 <- searchTwitter("Deadpool", n=18000, lang="en")     # Pull Twitter feed for Arsenal.
knitr::opts_chunk$set(echo = TRUE)
library(devtools)
library(Rfacebook)
library(scales)
library(reshape2)
library(corrplot)
library(textclean)
library(SnowballC)     # interface to the C libstemmer library that implements Porter's word stemming algorithm for collapsing words to a common root to aid comparison of vocabulary
library(twitteR)
library(tm)     	# text mining
library(syuzhet)     	# extracts sentiment and sentiment-derived plot arcs from text
library(sentimentr)     # sentiment analysis
library(wordcloud)
library(plyr)
library(ggplot2)
# Generate an access token to obtain your Twitter Access Token and Access Token Secret.
consumer_key    <- "BkiIHNzE2BvQhbRYVpkbZgGTA"
consumer_secret <-"zHAcSayUgKlfkksuf57oB0CMqFwDtgGI4TkDurXkEgNjHsqpnx"
access_token  <- "991503034650976257-W3MQKW99A0ec2pcm3kBw6ddXBIwkji5"
access_secret <- "bjhEqauHjM2gLPn7qUJvzThTWUW0vN3XVR8AkxsbphbD7"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)     # Setup Twitter open authorization. Select 2.
## Twitter Sentiment Analysis
tweet1 <- searchTwitter("Deadpool", n=17000, lang="en")     # Pull Twitter feed for Deadpool.
tweet1 <- twListToDF(tweet1)     # Convert into a data frame for analysis.
save(tweet1, file="Deadpool_05172018.RData")     # Save as RData file.
#load("tweet1.Rdata")
View(tweet1)
load("Deadpool_05172018.RData")
?load
load("Deadpool_05162018.RData")
DP_BR1 = tweet1
save(DP_BR1, file="Deadpool_05172018.RData")
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/Deadpool_05162018.RData")
## Twitter Sentiment Analysis
DP_051718 <- searchTwitter("Deadpool", n=17000, lang="en")     # Pull Twitter feed for Deadpool.
DP_051718 <- twListToDF(DP_051718)     # Convert into a data frame for analysis.
save(DP_051718, file="Deadpool_05172018.RData")     # Save as RData file.
#load("Deadpool_05172018.RData")
load("Deadpool_05162018.RData")
load("Deadpool_05162018.RData")
DP_051618 = tweet1
save(DP_051618, file="Deadpool_05162018.RData")
library(readr)
fandango <- read_csv("~/Documents/website/Sentiment-Analysis/fandango_score_comparison.csv")
View(fandango)
library(readr)
fandango <- read_csv("~/Documents/website/Sentiment-Analysis/fandango_score_comparison.csv")
corrplot(fandango, method="circle")
View(fandango)
t1 = fandango[,-1]
View(t1)
corrplot(t1, method="circle")
str(t1)
summary(t1)
corrplot(corr(t1), method="circle")
corrplot(cor(t1), method="circle")
fandango <- read_csv("~/Documents/website/Sentiment-Analysis/fandango_score_comparison.csv")
t1 = fandango[,-c(1,19:22)]
corrplot(cor(t1), method="circle")
load("Deadpool_05172018.RData")
load("Deadpool_05172018.RData")
tweet1 = DP_051718
head(tweet1)
colnames(tweet1)
twtxt <- tweet1$text     # Collect text for sentiment analysis.
class(twtxt)		# character
twtxt <- gsub("http://t.co/[a-z,A-Z,0-9]*{8}" , "", twtxt)     	# Clean up Tweets. Remove certain characters.
twtxt <- gsub("https://t.co/[a-z,A-Z,0-9]*{8}", "", twtxt)
# twtxt <- gsub("\\S+...", "", twtxt)
head(twtxt)
Encoding(twtxt) <- "latin1"
twtxt <- iconv(twtxt, from="latin1", to="ASCII", sub="")     	# Convert to ASCII format.
mycorpus <- Corpus(VectorSource(twtxt))     			# Create corpus.
mycorpus <- tm_map(mycorpus, content_transformer(tolower))	# Create corpus of text.
mycorpus <- tm_map(mycorpus, stripWhitespace)			# Remove white space.
mycorpus <- tm_map(mycorpus, removePunctuation)			# Remove punctuation.
mycorpus <- tm_map(mycorpus, removeNumbers)			# Remove numbers.
mycorpus <- tm_map(mycorpus, removeWords, stopwords())		# Remove stop words.
mycorpus <- tm_map(mycorpus, removeWords, c("deadpool"))	# Remove other words.
mycorpus[[3]]$content     # View the content of corpus.
wordcloud(mycorpus, scale=c(5, 0.5), max.words=100, random.order=F, rot.per=0.35, use.r.layout=F, colors=brewer.pal(8, "Dark2"))
final <- data.frame(text=sapply(mycorpus, identity), stringsAsFactors=F)
class(final)     # data frame
sent <- sentiment(final$text)     # Pull sentiment.
sent <- sentiment(final$text)     # Pull sentiment.
sent
mean(sent$sentiment)     # average sentiment
aggtwitter <- aggregate(sent$sentiment, by=list(sent$element_id), FUN=mean)
ggplot(data=aggtwitter, aes(x=Group.1, y=x)) + geom_bar(stat="identity") +
ggtitle("Sentiment per Tweet") + labs(x="Tweet", y="Sentiment Score")
sent2 <- get_sentiment(final$text, method="syuzhet")     # method="bing", "afinn", "nrc", "stanford"
sent3 <- syuzhet::get_nrc_sentiment(final$text)     # Get more detailed sentiment.
knitr::opts_chunk$set(echo = TRUE)
library(devtools)
library(Rfacebook)
library(scales)
library(reshape2)
library(corrplot)
library(textclean)
library(SnowballC)     # interface to the C libstemmer library that implements Porter's word stemming algorithm for collapsing words to a common root to aid comparison of vocabulary
library(twitteR)
library(tm)     	# text mining
library(syuzhet)     	# extracts sentiment and sentiment-derived plot arcs from text
library(sentimentr)     # sentiment analysis
library(wordcloud)
library(plyr)
library(ggplot2)
library(readr)
# Generate an access token to obtain your Twitter Access Token and Access Token Secret.
consumer_key    <- "BkiIHNzE2BvQhbRYVpkbZgGTA"
consumer_secret <-"zHAcSayUgKlfkksuf57oB0CMqFwDtgGI4TkDurXkEgNjHsqpnx"
access_token  <- "991503034650976257-W3MQKW99A0ec2pcm3kBw6ddXBIwkji5"
access_secret <- "bjhEqauHjM2gLPn7qUJvzThTWUW0vN3XVR8AkxsbphbD7"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)     # Setup Twitter open authorization. Select 2.
## Twitter Sentiment Analysis
DP_051818 <- searchTwitter("Deadpool", n=17000, lang="en")     # Pull Twitter feed for Deadpool.
DP_051818 <- twListToDF(DP_051818)     # Convert into a data frame for analysis.
save(DP_051818, file="DP_051818.RData")
knitr::opts_chunk$set(echo = TRUE)
library(devtools)
library(Rfacebook)
library(scales)
library(reshape2)
library(corrplot)
library(textclean)
library(SnowballC)     # interface to the C libstemmer library that implements Porter's word stemming algorithm for collapsing words to a common root to aid comparison of vocabulary
library(twitteR)
library(tm)     	# text mining
library(syuzhet)     	# extracts sentiment and sentiment-derived plot arcs from text
library(sentimentr)     # sentiment analysis
library(wordcloud)
library(plyr)
library(ggplot2)
library(readr)
# Generate an access token to obtain your Twitter Access Token and Access Token Secret.
consumer_key    <- "BkiIHNzE2BvQhbRYVpkbZgGTA"
consumer_secret <-"zHAcSayUgKlfkksuf57oB0CMqFwDtgGI4TkDurXkEgNjHsqpnx"
access_token  <- "991503034650976257-W3MQKW99A0ec2pcm3kBw6ddXBIwkji5"
access_secret <- "bjhEqauHjM2gLPn7qUJvzThTWUW0vN3XVR8AkxsbphbD7"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)     # Setup Twitter open authorization. Select 2.
## Twitter Sentiment Analysis
DP_051918 <- searchTwitter("Deadpool", n=17000, lang="en")     # Pull Twitter feed for Deadpool.
DP_051918 <- twListToDF(DP_051918)     # Convert into a data frame for analysis.
save(DP_051818, file="DP_051818.RData")
save(DP_051918, file="DP_051918.RData")
View(DP_051918)
knitr::opts_chunk$set(echo = TRUE)
library(devtools)
library(Rfacebook)
library(scales)
library(reshape2)
library(corrplot)
library(textclean)
library(SnowballC)     # interface to the C libstemmer library that implements Porter's word stemming algorithm for collapsing words to a common root to aid comparison of vocabulary
library(twitteR)
library(tm)     	# text mining
library(syuzhet)     	# extracts sentiment and sentiment-derived plot arcs from text
library(sentimentr)     # sentiment analysis
library(wordcloud)
library(plyr)
library(ggplot2)
library(readr)
# Generate an access token to obtain your Twitter Access Token and Access Token Secret.
consumer_key    <- "BkiIHNzE2BvQhbRYVpkbZgGTA"
consumer_secret <-"zHAcSayUgKlfkksuf57oB0CMqFwDtgGI4TkDurXkEgNjHsqpnx"
access_token  <- "991503034650976257-W3MQKW99A0ec2pcm3kBw6ddXBIwkji5"
access_secret <- "bjhEqauHjM2gLPn7qUJvzThTWUW0vN3XVR8AkxsbphbD7"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)     # Setup Twitter open authorization. Select 2.
load("Deadpool_05192018.RData")
load("DP_05192018.RData")
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/DP_051918.RData")
tweet1[3,]
tweet1 = DP_051918
tweet1[3,]
head(tweet1,3)
View(tweet1)
print(tweet1[3,])
print(tweet1[3,2])
print(tweet1[3,1])
knitr::opts_chunk$set(echo = TRUE)
library(devtools)
library(Rfacebook)
library(scales)
library(reshape2)
library(corrplot)
library(textclean)
library(SnowballC)     # interface to the C libstemmer library that implements Porter's word stemming algorithm for collapsing words to a common root to aid comparison of vocabulary
library(twitteR)
library(tm)     	# text mining
library(syuzhet)     	# extracts sentiment and sentiment-derived plot arcs from text
library(sentimentr)     # sentiment analysis
library(wordcloud)
library(plyr)
library(ggplot2)
library(readr)
# Generate an access token to obtain your Twitter Access Token and Access Token Secret.
consumer_key    <- "BkiIHNzE2BvQhbRYVpkbZgGTA"
consumer_secret <-"zHAcSayUgKlfkksuf57oB0CMqFwDtgGI4TkDurXkEgNjHsqpnx"
access_token  <- "991503034650976257-W3MQKW99A0ec2pcm3kBw6ddXBIwkji5"
access_secret <- "bjhEqauHjM2gLPn7qUJvzThTWUW0vN3XVR8AkxsbphbD7"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)     # Setup Twitter open authorization. Select 2.
fandango <- read_csv("~/Documents/website/Sentiment-Analysis/fandango_score_comparison.csv")
t1 = fandango[,-c(1,19:22)]
corrplot(cor(t1), method="circle")
Emery1 <- searchTwitter("Emery", n=5, lang="en", since = "2018-05-21", until = "2018-05-22")     # Pull Twitter feed for Deadpool.
Emery1 <- twListToDF(Emery1)     # Convert into a data frame for analysis.
View(Emery1)
## Twitter Sentiment Analysis
Emery1 <- searchTwitter("Emery", n=17000, lang="en", since = "2018-05-21", until = "2018-05-22")     # Pull Twitter feed for Deadpool.
Emery1 <- twListToDF(Emery1)     # Convert into a data frame for analysis.
save(Emery1, file="Emery1.RData")
View(t1)
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/DP_051918.RData")
tweet1 = DP_051918
print(tweet1[3,1])
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/DP_051618.RData")
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/DP_051718.RData")
View(DP_051918)
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/DP_051618.RData")
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/DP_051718.RData")
setwd("~/Documents/website/Sentiment-Analysis")
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/DP_051618.RData")
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/DP_051718.RData")
dir()
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/Deadpool_051618.RData")
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/Deadpool_051718.RData")
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/Deadpool_05162018.RData")
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/Deadpool_05172018.RData")
View(DP_051618)
t = merge(DP_051618,DP_051718)
t = rbind(DP_051618,DP_051718)
Deadpool1 = rbind(DP_051618,DP_051718)
View(Deadpool1)
View(Deadpool1)
print(Deadpool1[3,1])
View(Deadpool1)
print(Deadpool1[2,1])
head(Deadpool1)
head(Deadpool1[,-1])
str(Deadpool1)
summary(Deadpool1)
str(Deadpool1)
print(Deadpool1[2,1])
## Twitter Sentiment Analysis
Emery2 <- searchTwitter("Emery", n=17000, lang="en", since = "2018-05-23")     # Pull Twitter feed for Deadpool.
Emery2 <- twListToDF(Emery2)     # Convert into a data frame for analysis.
save(Emery2, file="Emery2.RData")
View(Emery2)
View(Emery1)
Emery2 <- searchTwitter("Emery", n=5, lang="en", since = "2018-05-20", until = "2018-05-21")     # Pull Twitter feed for Deadpool.
Emery2 <- twListToDF(Emery2)     # Convert into a data frame for analysis.
View(Emery2)
View(Emery2)
head(Deadpool1[,-1])
summary(Deadpool1)
print(Deadpool1[2,1])
View(Deadpool1)
twtxt <- Deadpool1$text     # Collect text for sentiment analysis.
class(twtxt)		# character
?gsub()
## Twitter Sentiment Analysis
Emery1 <- searchTwitter("Emery", n=18000, lang="en", since = "2018-05-20", until = "2018-05-21")     # Pull Twitter feed for before Emery appointment
Emery1 <- twListToDF(Emery1)     # Convert into a data frame for analysis.
save(Emery1, file="Emery1.RData")
View(Emery1)
## Twitter Sentiment Analysis
Emery2 <- searchTwitter("Emery", n=17000, lang="en", since = "2018-05-22", until = "2018-05-23")     # Pull Twitter feed for after Emery appointment.
Emery2 <- twListToDF(Emery2)     # Convert into a data frame for analysis.
save(Emery2, file="Emery2.RData")
View(Emery1)
## Twitter Sentiment Analysis
Emery1 <- searchTwitter("Emery", n=17000, lang="en", since = "2018-05-20", until = "2018-05-21")     # Pull Twitter feed for before Emery appointment
Emery1 <- twListToDF(Emery1)     # Convert into a data frame for analysis.
#save(Emery1, file="Emery1.RData")
twtxt[1]
twtxt[2]
twtxt <- Deadpool1$text     # Collect text for sentiment analysis.
class(twtxt)		# character
twtxt[2]
twtxt <- gsub("http://t.co/[a-z,A-Z,0-9]*{8}" , "", twtxt)     	# Clean up Tweets. Remove certain characters.
twtxt <- gsub("https://t.co/[a-z,A-Z,0-9]*{8}", "", twtxt)
twtxt[2]
twtxt <- gsub("\\S+...", "", twtxt)
twtxt[2]
head(twtxt)
twtxt[2]
twtxt <- Deadpool1$text     # Collect text for sentiment analysis.
twtxt[2]
twtxt <- gsub("http://t.co/[a-z,A-Z,0-9]*{8}" , "", twtxt)     	# Clean up Tweets. Remove certain characters.
twtxt <- gsub("https://t.co/[a-z,A-Z,0-9]*{8}", "", twtxt)
twtxt[2]
Encoding(twtxt) <- "latin1"
twtxt <- iconv(twtxt, from="latin1", to="ASCII", sub="")     	# Convert to ASCII format.
mycorpus <- Corpus(VectorSource(twtxt))     			# Create corpus.
mycorpus <- tm_map(mycorpus, content_transformer(tolower))	# Create corpus of text.
mycorpus <- tm_map(mycorpus, stripWhitespace)			# Remove white space.
mycorpus <- tm_map(mycorpus, removePunctuation)			# Remove punctuation.
mycorpus <- tm_map(mycorpus, removeNumbers)			# Remove numbers.
mycorpus <- tm_map(mycorpus, removeWords, stopwords())		# Remove stop words.
mycorpus <- tm_map(mycorpus, removeWords, c("deadpool"))	# Remove other words.
mycorpus[[3]]$content     # View the content of corpus.
mycorpus[[2]]$content     # View the content of corpus.
wordcloud(mycorpus, scale=c(5, 0.5), max.words=100, random.order=F, rot.per=0.35, use.r.layout=F, colors=brewer.pal(8, "Dark2"))
?searchTwitter
?Rtweets
?retweets
twtxt[2]
twtxt <- gsub("http://t.co/[a-z,A-Z,0-9]*{8}" , "", twtxt)     	# Clean up Tweets. Remove certain characters.
twtxt <- gsub("https://t.co/[a-z,A-Z,0-9]*{8}", "", twtxt)
twtxt[2]
twtxt2 <- gsub("^@.+", "", twtxt)
twtxt2[2]
twtxt2 <- gsub("^@", "", twtxt)
twtxt2[2]
twtxt2 <- gsub("@", "", twtxt)
twtxt2[2]
twtxt <- Deadpool1$text     # Collect text for sentiment analysis.
twtxt <- gsub("http://t.co/[a-z,A-Z,0-9]*{8}" , "", twtxt)     	# Clean up Tweets. Remove certain characters.
twtxt <- gsub("https://t.co/[a-z,A-Z,0-9]*{8}", "", twtxt)
twtxt[2]
test = twtxt[2]
test = twtxt[2]
result <- gsub("^@", "", test)
result[2]
result
?gsub()
result <- gsub("@.+", "", test)
result
result <- gsub("@", "", test)
result
result <- gsub("RT", "", test)
result
test = paste(twtxt[2],"rtesting")
result <- gsub("RT", "", test)
result
result
result <- gsub("@.", "", test)
result
result <- gsub("@.*", "", test)
result
twtxt[2]
result <- gsub("@.*:", "", test)
result
twtxt <- gsub("RT", "", twtxt) # Removes "RT" indicating a retweet
test = paste(twtxt[2],"rtesting")
result <- gsub("@.*:", "", test)
result
twtxt[2]
?Corpus
n
mycorpus[[2]]$content     # View the content of corpus.
final <- data.frame(text=sapply(mycorpus, identity), stringsAsFactors=F)
sent <- sentiment(final$text)     # Pull sentiment.
sent
sent
mean(sent$sentiment)     # average sentiment
sent
?sentiment
sent
mean(sent$sentiment)     # average sentiment
sent2 <- get_sentiment(final$text, method="syuzhet")     # method="bing", "afinn", "nrc", "stanford"
hist(sent2)
mean(sent2)     # average sentiment
# Sentiment per Tweet
aggtwitter <- aggregate(sent$sentiment, by=list(sent$element_id), FUN=mean)
ggplot(data=aggtwitter, aes(x=Group.1, y=x)) + geom_bar(stat="identity") +
ggtitle("Sentiment per Tweet") + labs(x="Tweet", y="Sentiment Score")
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/DP_051818.RData")
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/DP_051918.RData")
deadpool2 = rbind(DP_051818,DP_051918)
deadpool1 = rbind(DP_051618,DP_051718)
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/DP_051818.RData")
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/DP_051918.RData")
deadpool2 = rbind(DP_051818,DP_051918)
twtxt <- deadpool2$text
twtxt <- gsub("http://t.co/[a-z,A-Z,0-9]*{8}" , "", twtxt)     	# Clean up Tweets. Remove certain characters.
twtxt <- gsub("https://t.co/[a-z,A-Z,0-9]*{8}", "", twtxt)
twtxt <- gsub("RT", "", twtxt) # Removes "RT" indicating a retweet
twtxt <- gsub("@.*:", "", twtxt) # Removes screen names from retweets
Encoding(twtxt) <- "latin1"
twtxt <- iconv(twtxt, from="latin1", to="ASCII", sub="")     	# Convert to ASCII format.
mycorpus <- Corpus(VectorSource(twtxt))     			# Create corpus.
mycorpus <- tm_map(mycorpus, content_transformer(tolower))	# Create corpus of text.
mycorpus <- tm_map(mycorpus, stripWhitespace)			# Remove white space.
mycorpus <- tm_map(mycorpus, removePunctuation)			# Remove punctuation.
mycorpus <- tm_map(mycorpus, removeNumbers)			# Remove numbers.
mycorpus <- tm_map(mycorpus, removeWords, stopwords())		# Remove stop words.
mycorpus <- tm_map(mycorpus, removeWords, c("deadpool"))	# Remove other words.
# Perform the sentiment analysis on Tweets.
final <- data.frame(text=sapply(mycorpus, identity), stringsAsFactors=F)
sent <- sentiment(final$text)     # Pull sentiment.
sent
mean(sent$sentiment)     # average sentiment
head(deadpool1)
?hist()
sent2
df = data.frame(sent2)
View(df)
geom_histogram(df,aes(sent2))
View(df)
ggplot(df,aes(sent2))+geom_histogram()
View(aggtwitter)
ggplot(data=aggtwitter, aes(x))+geom_histogram()
ggplot(data=aggtwitter, aes(x=Group.1, y=x)) + geom_bar(stat="identity") +
ggtitle("Sentiment per Tweet") + labs(x="Tweet", y="Sentiment Score")
