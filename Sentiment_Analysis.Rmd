---
title: "Sentiment Analysis"
author: "Julio Gonzalez"
date: "5/8/2018"
output:
  html_document:
    css: bootstrap.min.css
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(devtools)
library(Rfacebook)
library(scales)  
library(reshape2) 
library(corrplot)
library(textclean)
library(SnowballC)     # interface to the C libstemmer library that implements Porter's word stemming algorithm for collapsing words to a common root to aid comparison of vocabulary
library(twitteR)
library(tm)     	# text mining
library(syuzhet)     	# extracts sentiment and sentiment-derived plot arcs from text
library(sentimentr)     # sentiment analysis
library(wordcloud)
library(plyr)   
library(ggplot2)
library(readr)

# Generate an access token to obtain your Twitter Access Token and Access Token Secret.
consumer_key    <- "BkiIHNzE2BvQhbRYVpkbZgGTA"
consumer_secret <-"zHAcSayUgKlfkksuf57oB0CMqFwDtgGI4TkDurXkEgNjHsqpnx"
access_token  <- "991503034650976257-W3MQKW99A0ec2pcm3kBw6ddXBIwkji5"
access_secret <- "bjhEqauHjM2gLPn7qUJvzThTWUW0vN3XVR8AkxsbphbD7"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)     # Setup Twitter open authorization. Select 2.
```

!Working Document!


Hi there, the Stats Whisper here, presenting a whole new topic that is a hot commodity these days which is social network analysis and sentiment analysis. Believe it or not but social media can have a powerful impact on a business. Can you imagine a company losing $1.3 billion (not a typo, as in 1,300,000,000 bucks) for a single tweet. Well that was what reportely happened to Snapchat Inc. when Kylie Jenner tweeted about snapchat alegedly dropping their stock value by 7.5%. 

The amount of data genereated by social networks is safe to say enourmous. What if we could use that same data to find insightful information on consumer perspective on just about anything. Well that's exactly what we are going to do. 

#Movie Reviews

Ever had a heated disscution with your friends about a particular movie or actor? While you have a strong distaste for anything Nicolas Cage, your "amigo" is in a infatuated trance with all of his movies. So what do you do when the next National Treseaure movie comes out? Naturally, you go, faute-de-mieux, online to investigate the reviews are for the movie. Well, I won't get into the particulars as to why of those movie reviews are flawed (actually [here](https://fivethirtyeight.com/features/ghostbusters-is-a-perfect-example-of-how-internet-ratings-are-broken/) is a very detailed article expanding on this issue from my favorite statisically inclined website FiveThirtyEight) but one salient issue is that these reviews are one dimensional. All they do is give a score from 1 to 10 or 1 to 5 stars for a particular movie but they don't tell you _why_ they are given this score. Furthermore, these movie rating sites are highly correlated with each and here we have some data to confirm that. 

```{r}
fandango <- read_csv("~/Documents/website/Sentiment-Analysis/fandango_score_comparison.csv")
t1 = fandango[,-c(1,19:22)]
corrplot(cor(t1), method="circle")
```

In the above diagram, we did a simple correlation plot between the movie ratings given by the major movie review sites and found all of them to be highly correlated with each other. For those non-stats heads out there, a correlation plot deploys a simple staticical algorithm known as the Pearson correlation value that finds how correlated two variables are to each other. There several ways this formula can be written but here is one that I think makes the more intuitive sense. $$r = \frac{\sum_{i = 1}^{n}(x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i = 1}^{n}(x_i - \bar{x})^2}\sqrt{\sum_{i = 1}^{n}(y_i - \bar{y})^2}}$$ The scale falls between -1 and 1 (inclusive) where a value of -1 means that the direc

I propose a solution: why don't we use the massive amount of user generated data of social networks to try and understand what are the reasons a user feels a certain way towards a movie. In addition, we might be able to extract what elements of movies makes it more likely for a good review.

Paradoxially, in order for someone to give a horror movie a high rating is by being scared out of your socks. 


##Deadpool

I will confess that I am a big superhero fan and will watch anything superhero related.


In order to conduct this analysis the most important thing we will need wil be the data. The great thing about Twitter is that it has an API (application programming interface) where instead of writing a script that will scrap the tweets from the webpage, we have direct access to the database where all the data on the tweets live. On top of that, there is an R package that has all the code wrapped up in a function that will do all of the leg work for you. All you have to do is pass the parameters for what you want to search for. Let's see how it works. 

```{r, eval=FALSE}
## Pulling Twitter Data 
Deadpool <- searchTwitter("Deadpool", n=34000, lang="en", since = "2018-05-16", until = "2018-05-17")     # Pull Twitter feed for Deadpool.
Deadpool1<- twListToDF(Deadpool)     # Convert into a data frame for analysis.
``` 

In the above code, we pulled 34,000 tweets containing the word "Deadpool" right up until the day before the premiere (movie debuted on May 18, 2018 in the US). Our goal is to analyze the sentiment before the movie was released to use it as a reference point and compare it to the sentiment after the movie premiered to see if we can find something there. 

```{r,include=FALSE, warning=FALSE}
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/Deadpool_05162018.RData")
load("/Users/juliogonzalez/Documents/website/Sentiment-Analysis/Deadpool_05172018.RData")
Deadpool1 = rbind(DP_051618,DP_051718)
``` 


Let's take a closer look at the data.
```{r}
head(Deadpool1[,-1])
summary(Deadpool1)
``` 

Here we see a summary statistics of the dataframe along with a snippet of the first 6 rows of the data. It starts off with text of the tweet followed by all sorts of information like when it was created, the device used to tweet, the screen name of the user, etc. We are primarily interest in the text of the tweet itself but this is just to show you that there the potential to conduct an even further analysis. 


```{r}
print(Deadpool1[2,1])
``` 

```{r}
twtxt <- Deadpool1$text     # Collect text for sentiment analysis.
class(twtxt)		# character
``` 

```{r}
twtxt <- gsub("http://t.co/[a-z,A-Z,0-9]*{8}" , "", twtxt)     	# Clean up Tweets. Remove certain characters.
twtxt <- gsub("https://t.co/[a-z,A-Z,0-9]*{8}", "", twtxt)
# twtxt <- gsub("\\S+...", "", twtxt)
head(twtxt)

Encoding(twtxt) <- "latin1"     				
twtxt <- iconv(twtxt, from="latin1", to="ASCII", sub="")     	# Convert to ASCII format.
mycorpus <- Corpus(VectorSource(twtxt))     			# Create corpus.
mycorpus <- tm_map(mycorpus, content_transformer(tolower))	# Create corpus of text.
mycorpus <- tm_map(mycorpus, stripWhitespace)			# Remove white space.
mycorpus <- tm_map(mycorpus, removePunctuation)			# Remove punctuation.
mycorpus <- tm_map(mycorpus, removeNumbers)			# Remove numbers.
mycorpus <- tm_map(mycorpus, removeWords, stopwords())		# Remove stop words. 
mycorpus <- tm_map(mycorpus, removeWords, c("deadpool"))	# Remove other words.
mycorpus[[3]]$content     # View the content of corpus.
``` 

```{r}
# Create word cloud to look for trends.
wordcloud(mycorpus, scale=c(5, 0.5), max.words=100, random.order=F, rot.per=0.35, use.r.layout=F, colors=brewer.pal(8, "Dark2"))

# Perform the sentiment analysis on Tweets.
final <- data.frame(text=sapply(mycorpus, identity), stringsAsFactors=F)
sent <- sentiment(final$text)     # Pull sentiment.
sent
mean(sent$sentiment)     # average sentiment

# Sentiment per Tweet
aggtwitter <- aggregate(sent$sentiment, by=list(sent$element_id), FUN=mean)
ggplot(data=aggtwitter, aes(x=Group.1, y=x)) + geom_bar(stat="identity") +
ggtitle("Sentiment per Tweet") + labs(x="Tweet", y="Sentiment Score") 

# The sentiment is positive.

sent2 <- get_sentiment(final$text, method="syuzhet")     # method="bing", "afinn", "nrc", "stanford" 
hist(sent2)
mean(sent2)     # average sentiment

# sent3 <- syuzhet::get_nrc_sentiment(final$text)     # Get more detailed sentiment.
# head(sent3)
# 
# sentsum <- colSums(sent3)
# barplot(sentsum	     , cex.names=0.5)
# barplot(sentsum[9:10], cex.names=0.5)
``` 


# Twitter Text Analysis

San Antonio
```{r, eval=FALSE}
avloc <- availableTrendLocations()
head(avloc)

# avloc contains information on the name of the location, the country, and its respective woeid (where on earth ID). 
# woeid is needed to get the trending topics of a chosen location at a given hour. 

avloc[avloc$name == "San Antonio",]

SA_ID <- avloc[avloc$name == "San Antonio",3]     # San Antonio's woeid is 2487796. 
SA_trends <- getTrends(woeid=SA_ID)     # Get trending topics in San Antonio.
head(SA_trends)
``` 

Houston
```{r, eval=FALSE}
HoustonID <- avloc[avloc$name == "Houston",3]
H_trends <- getTrends(woeid=HoustonID)     # Get trending topics in Houston.
head(H_trends)
``` 

Austin
```{r, eval=FALSE}
AustinID <- avloc[avloc$name == "Austin",3]
A_trends <- getTrends(woeid=AustinID)     # Get trending topics in Austin.
head(A_trends)
``` 

Dallas
```{r, eval=FALSE}
DallasID <- avloc[avloc$name == "Dallas-Ft. Worth",3] 
DFW_trends <- getTrends(woeid=DallasID)     # Get trending topics in Dallas-Ft.Worth.
head(DFW_trends)
``` 


##Arsenal
```{r, eval=FALSE}
## Twitter Sentiment Analysis
Emery1 <- searchTwitter("Emery", n=18000, lang="en", since = "2018-05-20", until = "2018-05-21")     # Pull Twitter feed for before Emery appointment
Emery1 <- twListToDF(Emery1)     # Convert into a data frame for analysis.
save(Emery1, file="Emery1.RData")
``` 

```{r, eval=FALSE}
## Twitter Sentiment Analysis
Emery2 <- searchTwitter("Emery", n=18000, lang="en", since = "2018-05-22", until = "2018-05-23")     # Pull Twitter feed for after Emery appointment.
Emery2 <- twListToDF(Emery2)     # Convert into a data frame for analysis.
save(Emery2, file="Emery2.RData")
``` 
