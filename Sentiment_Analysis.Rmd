---
title: "Sentiment Analysis"
author: "Julio Gonzalez"
date: "5/8/2018"
output:
  html_document:
    css: bootstrap.min.css
    toc: true
    toc_depth: 2
    toc_float:
      collapsed: false
      smooth_scroll: true
---

```{r setup, include=FALSE, warning=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(devtools)
library(Rfacebook)
library(scales)  
library(reshape2) 
library(corrplot)
library(textclean)
library(SnowballC)     # interface to the C libstemmer library that implements Porter's word stemming algorithm for collapsing words to a common root to aid comparison of vocabulary
library(twitteR)
library(tm)     	# text mining
library(syuzhet)     	# extracts sentiment and sentiment-derived plot arcs from text
library(sentimentr)     # sentiment analysis
library(wordcloud)
library(plyr)   
library(ggplot2)
library(readr)

# Generate an access token to obtain your Twitter Access Token and Access Token Secret.
consumer_key    <- "BkiIHNzE2BvQhbRYVpkbZgGTA"
consumer_secret <-"zHAcSayUgKlfkksuf57oB0CMqFwDtgGI4TkDurXkEgNjHsqpnx"
access_token  <- "991503034650976257-W3MQKW99A0ec2pcm3kBw6ddXBIwkji5"
access_secret <- "bjhEqauHjM2gLPn7qUJvzThTWUW0vN3XVR8AkxsbphbD7"
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)     # Setup Twitter open authorization. Select 2.
```

!Working Document!


Hi there, the Stats Whisper here, presenting a whole new topic that is a hot commodity these day which is social network analysis and sentiment analysis. Believe it or not but social media can have a powerful impact on a business. Can you imagine a company losing $1.3 billion (not a typo, as in 1,300,000,000 bucks) for a single tweet. Well that was what reportely happened to Snapchat Inc. when Kylie Jenner tweeted something about snapchat dropping their stock value by 7.5%. 

The amount of data genereated by social networks is safe to say enourmous. What if we could use that same data to find insight information on movies. That's exactly what we are going to do. 

#Movie Reviews

Ever had a heated disscution with your friends about a particular movie or actor? While you have a strong distaste for anything Nicolas Cage, your "amigo" is in a infatuated trance with all of his movies. So what do you do when the next National Treseaure movie comes out? Naturally, you go online and look at the reviews to see what the reviews are the movie. Well, I won't get in the particulars as to why of those movie reviews are flawed (actually ([here](https://fivethirtyeight.com/features/ghostbusters-is-a-perfect-example-of-how-internet-ratings-are-broken/)) is a very detailed article expanding on this issue from my favorite statisically inclined website FiveThirtyEight) but one salient issue is that these reviews are one dimensional. All they do is give a score from 1 to 10 or 1 to 5 stars for a particular movie but they don't tell you _why_ they are given this score. I propose a solution: why don't use the massive amount of user generated data of social networks to try and understand what are the reasons a user feels a certain way towards a movie. In addition, we might be able to extract what elements of movies makes it more likely for a good review.

```{r}
fandango <- read_csv("~/Documents/website/Sentiment-Analysis/fandango_score_comparison.csv")
t1 = fandango[,-c(1,19:22)]
corrplot(cor(t1), method="circle")
```

Here we see that the scores across the different rating websites are correlated. 


Paradoxially, in order for someone to give a horror movie a high rating is by being scared out of your socks. 



# Twitter Text Analysis

San Antonio
```{r}
avloc <- availableTrendLocations()
head(avloc)

# avloc contains information on the name of the location, the country, and its respective woeid (where on earth ID). 
# woeid is needed to get the trending topics of a chosen location at a given hour. 

avloc[avloc$name == "San Antonio",]

SA_ID <- avloc[avloc$name == "San Antonio",3]     # San Antonio's woeid is 2487796. 
SA_trends <- getTrends(woeid=SA_ID)     # Get trending topics in San Antonio.
head(SA_trends)
``` 

Houston
```{r}
HoustonID <- avloc[avloc$name == "Houston",3]
H_trends <- getTrends(woeid=HoustonID)     # Get trending topics in Houston.
head(H_trends)
``` 

Austin
```{r}
AustinID <- avloc[avloc$name == "Austin",3]
A_trends <- getTrends(woeid=AustinID)     # Get trending topics in Austin.
head(A_trends)
``` 

Dallas
```{r}
DallasID <- avloc[avloc$name == "Dallas-Ft. Worth",3] 
DFW_trends <- getTrends(woeid=DallasID)     # Get trending topics in Dallas-Ft.Worth.
head(DFW_trends)
``` 



##Deadpool

I will confess that I am a big superhero fan and will watch anything 



```{r, eval=FALSE}
## Twitter Sentiment Analysis
DP_051718 <- searchTwitter("Deadpool", n=17000, lang="en")     # Pull Twitter feed for Deadpool.
DP_051718 <- twListToDF(DP_051718)     # Convert into a data frame for analysis. 
``` 

```{r}
load("Deadpool_05172018.RData")
tweet1 = DP_051718

head(tweet1)
colnames(tweet1)

twtxt <- tweet1$text     # Collect text for sentiment analysis.
class(twtxt)		# character

twtxt <- gsub("http://t.co/[a-z,A-Z,0-9]*{8}" , "", twtxt)     	# Clean up Tweets. Remove certain characters.
twtxt <- gsub("https://t.co/[a-z,A-Z,0-9]*{8}", "", twtxt)
# twtxt <- gsub("\\S+...", "", twtxt)
head(twtxt)

Encoding(twtxt) <- "latin1"     				
twtxt <- iconv(twtxt, from="latin1", to="ASCII", sub="")     	# Convert to ASCII format.
mycorpus <- Corpus(VectorSource(twtxt))     			# Create corpus.
mycorpus <- tm_map(mycorpus, content_transformer(tolower))	# Create corpus of text.
mycorpus <- tm_map(mycorpus, stripWhitespace)			# Remove white space.
mycorpus <- tm_map(mycorpus, removePunctuation)			# Remove punctuation.
mycorpus <- tm_map(mycorpus, removeNumbers)			# Remove numbers.
mycorpus <- tm_map(mycorpus, removeWords, stopwords())		# Remove stop words. 
mycorpus <- tm_map(mycorpus, removeWords, c("deadpool"))	# Remove other words.
mycorpus[[3]]$content     # View the content of corpus.
``` 

```{r}
# Create word cloud to look for trends.
wordcloud(mycorpus, scale=c(5, 0.5), max.words=100, random.order=F, rot.per=0.35, use.r.layout=F, colors=brewer.pal(8, "Dark2"))

# Perform the sentiment analysis on Tweets.
final <- data.frame(text=sapply(mycorpus, identity), stringsAsFactors=F)
sent <- sentiment(final$text)     # Pull sentiment.
sent
mean(sent$sentiment)     # average sentiment

# Sentiment per Tweet
aggtwitter <- aggregate(sent$sentiment, by=list(sent$element_id), FUN=mean)
ggplot(data=aggtwitter, aes(x=Group.1, y=x)) + geom_bar(stat="identity") +
ggtitle("Sentiment per Tweet") + labs(x="Tweet", y="Sentiment Score") 

# The sentiment is positive.

sent2 <- get_sentiment(final$text, method="syuzhet")     # method="bing", "afinn", "nrc", "stanford" 
hist(sent2)
mean(sent2)     # average sentiment

# sent3 <- syuzhet::get_nrc_sentiment(final$text)     # Get more detailed sentiment.
# head(sent3)
# 
# sentsum <- colSums(sent3)
# barplot(sentsum	     , cex.names=0.5)
# barplot(sentsum[9:10], cex.names=0.5)
``` 
